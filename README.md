# KDD 2025 Tutorial: Evaluation & Benchmarking of LLM Agents

---

## üìÑ Abstract

The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This tutorial provides a systematic survey of the field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along:
- **Evaluation objectives** (what to evaluate): agent behavior, capabilities, reliability, safety
- **Evaluation process** (how to evaluate): interaction modes, datasets and benchmarks, metric computation methods, and tooling

In addition, we highlight enterprise-specific challenges, such as role-based access, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance. Finally, we discuss future research directions toward holistic, more realistic, and scalable evaluation of LLM agents.

---

## üéØ Target Audience

This tutorial is designed for applied and industry data scientists, machine learning engineers, and enterprise AI practitioners who build or deploy LLM-based agents in production systems. It is also relevant for academic researchers studying evaluation methodologies, multi-agent systems, and trustworthy language models. Participants will gain a systematic evaluation framework, practical hands-on code examples, and insights into real-world deployment challenges.

---

## üóÇÔ∏è Tutorial Agenda

- **Introduction (5 min)**
  - Motivation and tutorial goals
- **Taxonomy Overview (10 min)**
  - What to evaluate and how to evaluate
- **Evaluation Process (15 min)**
  - Interaction modes
  - Evaluation data
  - Metric computation methods
  - Evaluation tooling
  - Evaluation contexts
- **Evaluation Objectives (75 min)**
  - Agent Behavior
  - Agent Capabilities
  - Reliability
  - Safety & Alignment
- **Enterprise-Specific Challenges (20 min)**
  - Access control
  - Reliability guarantees
  - Dynamic & long-horizon interactions
  - Policy and compliance
- **Future Directions (10 min)**
  - Holistic frameworks
  - Scalable evaluation
  - Realistic enterprise settings

---

## üë©‚Äçüíª Authors

- **Mahmoud Mohammadi** ‚Äî SAP Labs 
  *Mahmoud is a Senior AI Scientist at SAP, where his research focuses on business foundation models and agentic AI, including graph foundation models, LLM integration, and the evaluation of intelligent agents. He also has expertise in Generative Adversarial Networks (GANs) and multimodal AI systems. Previously, Mahmoud worked at Microsoft, where he contributed to developing client-side deep learning models for Windows. He holds a Ph.D. in Computer Science form university of North Carolina at Charlotte.*  
  [mahmoud.mohammadi@sap.com](mailto:mahmoud.mohammadi@sap.com)

- **Yipeng Li** ‚Äî SAP Labs, Bellevue, WA, USA  
  *Yipeng Li is a Data Scientist Expert at SAP, leading research and development in agentic AI. His work focuses on single and multi-agent systems, quality assessment, and enabling internal AI research and development through common platforms.*  
  [yipeng.li@sap.com](mailto:yipeng.li@sap.com)

- **Jane Lo** ‚Äî SAP Labs, Palo Alto, CA, USA  
  *Jane Lo is an AI Scientist at SAP, focusing on the research and development of agentic AI. She has worked on several projects in the field, focusing on the integration of enterprise tools, data, and private knowledge with agentic systems across a wide range of conversational and autonomous use cases.*  
  [jane.lo@sap.com](mailto:jane.lo@sap.com)

- **Wendy Yip** ‚Äî SAP Labs, Palo Alto, CA, USA  
  *Wendy is a Senior Data Scientist in SAP and has a background in astrophysics and spent time on machine learning \& data-intensive science research at Johns Hopkins University.*  
  [wendy.yip@sap.com](mailto:wendy.yip@sap.com)

---

## üöÄ License

This content is provided for educational and tutorial purposes under the SAP standard tutorial license.  
See [SAP Legal Notices](https://www.sap.com/about/legal.html) for details.

---

